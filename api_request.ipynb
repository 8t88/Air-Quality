{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4a1495-514c-4a9c-88c4-f095b7692772",
   "metadata": {},
   "source": [
    "# This notebook was adapted from the [instructions](https://disc.gsfc.nasa.gov/information/howto?keywords=MERRA&title=How%20to%20Use%20the%20Web%20Services%20API%20for%20Subsetting%20MERRA-2%20Data) found on the NASA Earthdata website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f8f4e0-5293-4014-8176-13bb74a2f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import urllib3\n",
    "import certifi\n",
    "import requests\n",
    "from time import sleep\n",
    "from http.cookiejar import CookieJar\n",
    "import urllib.request\n",
    "from urllib.parse import urlencode\n",
    "import getpass\n",
    "import xarray as xr\n",
    "\n",
    "    ##helpers\n",
    "\n",
    "def get_http_data(request):\n",
    "\n",
    "    # Create a urllib PoolManager instance to make requests.\n",
    "    http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())\n",
    "    # Set the URL for the GES DISC subset service endpoint\n",
    "    url = 'https://disc.gsfc.nasa.gov/service/subset/jsonwsp'\n",
    "\n",
    "    hdrs = {'Content-Type': 'application/json',\n",
    "            'Accept'      : 'application/json'}\n",
    "    data = json.dumps(request)       \n",
    "    r = http.request('POST', url, body=data, headers=hdrs)\n",
    "    response = json.loads(r.data)   \n",
    "    # Check for errors\n",
    "    if response['type'] == 'jsonwsp/fault' :\n",
    "        print('API Error: faulty %s request' % response['methodname'])\n",
    "        sys.exit(1)\n",
    "    return response\n",
    "\n",
    "def agg_long_lat(xarr_wide):\n",
    "    weights = np.cos(np.deg2rad(xarr_wide.lat))\n",
    "    xarr_weighted = xarr_wide.weighted(weights)\n",
    "    xarr_flat = xarr_weighted.mean(dim=[\"lat\", \"lon\"])\n",
    "    return xarr_flat\n",
    "\n",
    "def open_ncfile(filename_arr):\n",
    "\n",
    "    aq_xarr = xr.open_dataset(filename_arr)\n",
    "    aq_data_pm = aq_xarr.assign(pm25=aq_xarr['DUSMASS25']+aq_xarr['SSSMASS25']+aq_xarr['BCSMASS']+aq_xarr['OCSMASS']+1.375*aq_xarr['SO4SMASS']*1000000)\n",
    "    return aq_data_pm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e795b-d837-4ac3-bb11-1deffa85c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def get_pm25_data(username, pw, start_date, end_date, minlong, maxlong, minlat, maxlat, product = 'M2TUNXAER_5.12.4'):\n",
    "    varNames =['OCSMASS', 'BCSMASS', 'SO4SMASS', 'DUSMASS25', 'SSSMASS25']  \n",
    "    end_date = (datetime.strptime(end_date, \"%Y-%m-%d\")-relativedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    subset_request = {\n",
    "        'methodname': 'subset',\n",
    "        'type': 'jsonwsp/request',\n",
    "        'version': '1.0',\n",
    "        'args': {\n",
    "            'role'  : 'subset',\n",
    "            'start' : start_date,\n",
    "            'end'   : end_date,\n",
    "            'box'   : [minlong, minlat, maxlong, maxlat],\n",
    "            'crop'  : True, \n",
    "    #        'mapping': interp,\n",
    "    #        'grid'  : destGrid,\n",
    "            'data': [{'datasetId': product,\n",
    "                      'variable' : varNames[0]\n",
    "                     },\n",
    "                      {'datasetId': product,\n",
    "                      'variable' : varNames[1]\n",
    "                     },\n",
    "                     {'datasetId': product,\n",
    "                      'variable' : varNames[2]\n",
    "                     },\n",
    "                      {'datasetId': product,\n",
    "                      'variable' : varNames[3]\n",
    "                     },\n",
    "                     {'datasetId': product,\n",
    "                      'variable' : varNames[4]                 \n",
    "                     }]\n",
    "               }\n",
    "    }\n",
    "\n",
    "    # Submit the subset request to the GES DISC Server\n",
    "    response = get_http_data(subset_request)\n",
    "    # Report the JobID and initial status\n",
    "    myJobId = response['result']['jobId']\n",
    "    print('Job ID: '+myJobId)\n",
    "    print('Job status: '+response['result']['Status'])\n",
    "\n",
    "    # Construct JSON WSP request for API method: GetStatus\n",
    "    status_request = {\n",
    "        'methodname': 'GetStatus',\n",
    "        'version': '1.0',\n",
    "        'type': 'jsonwsp/request',\n",
    "        'args': {'jobId': myJobId}\n",
    "    }\n",
    "    \n",
    "    # Check on the job status after a brief nap\n",
    "    while response['result']['Status'] in ['Accepted', 'Running']:\n",
    "        sleep(5)\n",
    "        response = get_http_data(status_request)\n",
    "        status  = response['result']['Status']\n",
    "        percent = response['result']['PercentCompleted']\n",
    "        print ('Job status: %s (%d%c complete)' % (status,percent,'%'))\n",
    "    if response['result']['Status'] == 'Succeeded' :\n",
    "        print ('Job Finished:  %s' % response['result']['message'])\n",
    "    else : \n",
    "        print('Job Failed: %s' % response['fault']['code'])\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Construct JSON WSP request for API method: GetResult\n",
    "    batchsize = 20\n",
    "    results_request = {\n",
    "        'methodname': 'GetResult',\n",
    "        'version': '1.0',\n",
    "        'type': 'jsonwsp/request',\n",
    "        'args': {\n",
    "            'jobId': myJobId,\n",
    "            'count': batchsize,\n",
    "            'startIndex': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Retrieve the results in JSON in multiple batches \n",
    "    # Initialize variables, then submit the first GetResults request\n",
    "    # Add the results from this batch to the list and increment the count\n",
    "    results = []\n",
    "    count = 0 \n",
    "    response = get_http_data(results_request) \n",
    "    count = count + response['result']['itemsPerPage']\n",
    "    results.extend(response['result']['items']) \n",
    "    \n",
    "    # Increment the startIndex and keep asking for more results until we have them all\n",
    "    total = response['result']['totalResults']\n",
    "    while count < total :\n",
    "        results_request['args']['startIndex'] += batchsize \n",
    "        response = get_http_data(results_request) \n",
    "        count = count + response['result']['itemsPerPage']\n",
    "        results.extend(response['result']['items'])\n",
    "           \n",
    "    # Check on the bookkeeping\n",
    "    print('Retrieved %d out of %d expected items' % (len(results), total))\n",
    "\n",
    "    # Sort the results into documents and URLs\n",
    "    docs = []     # documentation\n",
    "    urls = []     # data URLs\n",
    "    for item in results :\n",
    "        try:\n",
    "            if item['start'] and item['end'] : urls.append(item) \n",
    "        except:\n",
    "            docs.append(item)\n",
    "    # Print out the documentation links, but do not download them\n",
    "    print('\\nDocumentation:')\n",
    "    for item in docs : print(item['label']+': '+item['link'])\n",
    "    \n",
    "    # Create a password manager to deal with the 401 response that is returned from\n",
    "    # Earthdata Login\n",
    "    \n",
    "    password_manager = urllib.request.HTTPPasswordMgrWithDefaultRealm()\n",
    "    password_manager.add_password(None, \"https://urs.earthdata.nasa.gov\", username, pw)\n",
    "    \n",
    "    # Create a cookie jar for storing cookies. This is used to store and return the session cookie #given to use by the data server\n",
    "    cookie_jar = CookieJar()\n",
    "       \n",
    "    # Install all the handlers.\n",
    "    opener = urllib.request.build_opener (urllib.request.HTTPBasicAuthHandler (password_manager),urllib.request.HTTPCookieProcessor (cookie_jar))\n",
    "    urllib.request.install_opener(opener)\n",
    "     \n",
    "    # Open a request for the data, and download files\n",
    "    print('\\nHTTP_services output:')\n",
    "    saved_files = []    \n",
    "    for item in urls:\n",
    "        URL = item['link'] \n",
    "        DataRequest = urllib.request.Request(URL)\n",
    "        DataResponse = urllib.request.urlopen(DataRequest)\n",
    "    \n",
    "        DataBody = DataResponse.read()     # Print out the result\n",
    "    \n",
    "    # Save file to working directory\n",
    "        try:\n",
    "            file_name = \"./data/\" + item['label']\n",
    "            file_ = open(file_name, 'wb')\n",
    "            file_.write(DataBody)\n",
    "            file_.close()\n",
    "            print (file_name, \"is downloaded\")\n",
    "            saved_files.append(file_name)\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "             print(e)\n",
    "                \n",
    "    print('Downloading is done and find the downloaded files in your current working directory')    \n",
    "\n",
    "    #dataset = convert_ncfile(saved_files)\n",
    "    \n",
    "    return saved_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dddcf4-2030-42d3-ae0f-87c76bf9c489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fb2670-b883-4419-b0f7-ffa34873aa3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
